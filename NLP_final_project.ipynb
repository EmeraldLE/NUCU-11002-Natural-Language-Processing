{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31331ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本數： 84169\n",
      "驗證樣本數： 12078\n",
      "測試樣本數： 10973\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 讀取資料集\n",
    "train = pd.read_csv(\"train/fixed_train.csv\")\n",
    "valid = pd.read_csv(\"train/fixed_valid.csv\")\n",
    "test = pd.read_csv(\"train/fixed_test.csv\")\n",
    "\n",
    "# data cleaning about _comma_\n",
    "train['prompt'] = train['prompt'].map(lambda x:x.replace('_comma_',','))\n",
    "train['utterance'] = train['utterance'].map(lambda x:x.replace('_comma_',','))\n",
    "valid['prompt'] = valid['prompt'].map(lambda x:x.replace('_comma_',','))\n",
    "valid['utterance'] = valid['utterance'].map(lambda x:x.replace('_comma_',','))\n",
    "test['prompt'] = test['prompt'].map(lambda x:x.replace('_comma_',','))\n",
    "test['utterance'] = test['utterance'].map(lambda x:x.replace('_comma_',','))\n",
    "\n",
    "print(\"訓練樣本數：\", len(train))\n",
    "print(\"驗證樣本數：\", len(valid))\n",
    "print(\"測試樣本數：\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5debf988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words(\"english\")\n",
    "\n",
    "#get pos\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#lemmatizer\n",
    "def lemmas(text):\n",
    "    #token\n",
    "    token = nltk.tokenize.word_tokenize(text.lower())\n",
    "    \n",
    "    #lemmatize        \n",
    "    tagged_sent = pos_tag(token)#get pos     \n",
    "    wnl = WordNetLemmatizer()#init lemmatizer\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) \n",
    "    \n",
    "    #delete stopword\n",
    "    lemmas_sent = [e for e in lemmas_sent if e not in stoplist]\n",
    "\n",
    "    return ' '.join(lemmas_sent)\n",
    "\n",
    "def del_stopword(text):\n",
    "    #token\n",
    "    token = nltk.tokenize.word_tokenize(text.lower())\n",
    "    #delete stopword\n",
    "    token = [e for e in token if e not in stoplist]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5ed080",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prompt'] = train['prompt'].apply(lemmas)\n",
    "train['utterance'] = train['utterance'].apply(lemmas)\n",
    "valid['prompt'] = valid['prompt'].apply(lemmas)\n",
    "valid['utterance'] = valid['utterance'].apply(lemmas)\n",
    "test['prompt'] = test['prompt'].apply(lemmas)\n",
    "test['utterance'] = test['utterance'].apply(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c2031a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_label = {'sad': 0, 'trusting': 1, 'terrified': 2, 'caring': 3, 'disappointed': 4,\n",
    "         'faithful': 5, 'joyful': 6, 'jealous': 7, 'disgusted': 8, 'surprised': 9,\n",
    "         'ashamed': 10, 'afraid': 11, 'impressed': 12, 'sentimental': 13, \n",
    "         'devastated': 14, 'excited': 15, 'anticipating': 16, 'annoyed': 17, 'anxious': 18,\n",
    "         'furious': 19, 'content': 20, 'lonely': 21, 'angry': 22, 'confident': 23,\n",
    "         'apprehensive': 24, 'guilty': 25, 'embarrassed': 26, 'grateful': 27,\n",
    "         'hopeful': 28, 'proud': 29, 'prepared': 30, 'nostalgic': 31}\n",
    "len(emotion_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4d5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, df, tokenizer):\n",
    "        assert mode in [\"train\", \"valid\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = df.fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = emotion_label\n",
    "        self.tokenizer = tokenizer\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            prompt, utterance = self.df.iloc[idx, 2:].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            prompt, utterance, label = self.df.iloc[idx, 2:].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "        \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_prompt = self.tokenizer.tokenize(prompt)\n",
    "        word_pieces += tokens_prompt + [\"[SEP]\"]\n",
    "        len_prompt = len(word_pieces)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_utterance = self.tokenizer.tokenize(utterance)\n",
    "        word_pieces += tokens_utterance + [\"[SEP]\"]\n",
    "        len_utterance = len(word_pieces) - len_prompt\n",
    "        \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        segments_tensor = torch.tensor([0] * len_prompt + [1] * len_utterance, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# 初始化 Dataset\n",
    "trainset = EmotionDataset(\"train\", train, tokenizer=tokenizer)\n",
    "validset = EmotionDataset(\"valid\", valid, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0c9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,\n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=128, \n",
    "                         collate_fn=create_mini_batch)\n",
    "\n",
    "validloader = DataLoader(validset, batch_size=128, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d2c82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入一個預訓練好可以做多分類任務的模型，n_class = 32\n",
    "from transformers import BertForSequenceClassification\n",
    "from IPython.display import clear_output\n",
    "\n",
    "NUM_LABELS = 32\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f9e4f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "CPU times: user 1.24 s, sys: 778 ms, total: 2.02 s\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "                \n",
    "            if not compute_acc:\n",
    "                # 只是單純要回傳預測值的話，不用計算準確度也不用紀錄 loss\n",
    "                tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "                outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "                logits = outputs[0]\n",
    "                _, pred = torch.max(logits.data, 1)\n",
    "            else:\n",
    "                # 否則就要計算 loss，這邊有一個小細節是 model 如果有吃 label 的話，\n",
    "                # output[0]會變成是 loss，沒有吃 label 時 output[0] 會是 logits\n",
    "                tokens_tensors, segments_tensors, masks_tensors, labels = data[:4]\n",
    "                outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors,\n",
    "                                labels=labels)\n",
    "                loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "                _, pred = torch.max(logits.data, 1)\n",
    "                running_loss += loss.item()\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "                \n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        loss = running_loss / total\n",
    "        return predictions, acc, loss\n",
    "    \n",
    "    return predictions\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ff2d9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f7ccf3b99c4ff7a991f6494c2b4be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8546713f6d4decbb9e58533b30e723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694c8f84b17c48bfbd286f1449742528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train loss: 0.011, train acc: 0.593, valid loss: 0.012, valid acc: 0.542\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d59f54f9704e0ba48429abd5919f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc909366a43a44e2ab5dbdbcdcea73a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce731d549b0446599e862d39803a429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] train loss: 0.008, train acc: 0.668, valid loss: 0.011, valid acc: 0.562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097c41730c684032a07437e5d1cb15dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5820b3861b6428ab7b1dea78cfc7d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8181516f6ef04358b0d80ee659c07729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] train loss: 0.007, train acc: 0.711, valid loss: 0.011, valid acc: 0.570\n",
      "CPU times: user 25min 35s, sys: 9min 43s, total: 35min 18s\n",
      "Wall time: 35min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "# Create the learning rate scheduler.\n",
    "total_steps = len(trainloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(trainloader):\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 計算分類準確率\n",
    "    _, train_acc, train_loss = get_predictions(model, trainloader, compute_acc=True)\n",
    "    _, valid_acc, valid_loss = get_predictions(model, validloader, compute_acc=True)\n",
    "    \n",
    "    print('[epoch %d] train loss: %.3f, train acc: %.3f, valid loss: %.3f, valid acc: %.3f' %\n",
    "          (epoch + 1, train_loss, train_acc, valid_loss, valid_acc))\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': train_loss,\n",
    "            'Training Accur.': train_acc,\n",
    "            'Valid. Loss': valid_loss,\n",
    "            'Valid. Accur.': valid_acc\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabb50e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d7db9_ td {\n",
       "  max-width: 70px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d7db9_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Training Loss</th>\n",
       "      <th class=\"col_heading level0 col1\" >Training Accur.</th>\n",
       "      <th class=\"col_heading level0 col2\" >Valid. Loss</th>\n",
       "      <th class=\"col_heading level0 col3\" >Valid. Accur.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >epoch</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d7db9_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_d7db9_row0_col0\" class=\"data row0 col0\" >0.011</td>\n",
       "      <td id=\"T_d7db9_row0_col1\" class=\"data row0 col1\" >0.593</td>\n",
       "      <td id=\"T_d7db9_row0_col2\" class=\"data row0 col2\" >0.012</td>\n",
       "      <td id=\"T_d7db9_row0_col3\" class=\"data row0 col3\" >0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d7db9_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_d7db9_row1_col0\" class=\"data row1 col0\" >0.008</td>\n",
       "      <td id=\"T_d7db9_row1_col1\" class=\"data row1 col1\" >0.668</td>\n",
       "      <td id=\"T_d7db9_row1_col2\" class=\"data row1 col2\" >0.011</td>\n",
       "      <td id=\"T_d7db9_row1_col3\" class=\"data row1 col3\" >0.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d7db9_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_d7db9_row2_col0\" class=\"data row2 col0\" >0.007</td>\n",
       "      <td id=\"T_d7db9_row2_col1\" class=\"data row2 col1\" >0.711</td>\n",
       "      <td id=\"T_d7db9_row2_col2\" class=\"data row2 col2\" >0.011</td>\n",
       "      <td id=\"T_d7db9_row2_col3\" class=\"data row2 col3\" >0.570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff5ff0f17f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('precision', 3)\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats = df_stats.style.set_table_styles([dict(selector=\"td\",props=[('max-width', '70px')])])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315ec274",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9954f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=NUM_LABELS)\n",
    "model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea25343c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f48b1256cf45aba4d9c001e1ee9eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testset = EmotionDataset(\"test\", test, tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=128, \n",
    "                        collate_fn=create_mini_batch)\n",
    "predictions = get_predictions(model, testloader)\n",
    "predictions = predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524f0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "conv_id_clean_test_list = list(dict.fromkeys(test.conv_id.tolist()))\n",
    "conv_id_test_list = test.conv_id.tolist()\n",
    "output = []\n",
    "index = 0\n",
    "for ids in conv_id_clean_test_list:\n",
    "    temp = []\n",
    "    for i in range(conv_id_test_list.count(ids)):\n",
    "        temp.append(predictions[index])\n",
    "        index+=1\n",
    "    for i in range(conv_id_test_list.count(ids)):\n",
    "        output.append(most_common(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713e55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = dict(enumerate(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9042409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['', 'pred']\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(fieldnames)\n",
    "    for k, v in test_dict.items():\n",
    "        writer.writerow([k, v])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
