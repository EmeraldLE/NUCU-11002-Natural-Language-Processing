{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4-J5jkcatiQ"
      },
      "source": [
        "**Lab1 Reminder: **\n",
        "1. Use small data to develop your code.\n",
        "2. Relink to the demomstration code can check the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VISir4btCqU9",
        "outputId": "decf0787-47a4-41e3-a385-6863f18d897a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqVxwKpgCe0a",
        "outputId": "053757a1-6991-490d-cdb0-5fabe2e25786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# For debugging\n",
        "import pdb\n",
        "\n",
        "# For checking progress\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For loading data\n",
        "import pandas as pd\n",
        "\n",
        "# For tokenizaton\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# For building n-gram model\n",
        "from collections import Counter, namedtuple\n",
        "import numpy as np\n",
        "\n",
        "# For evaluation \n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge import Rouge \n",
        "\n",
        "# For pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4r6vYRwwV6m"
      },
      "source": [
        "# Part 1. Data Preprocessing\n",
        "1. show the top-10 common words and their counts before/after preprocessing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXl7m78Oxz9d"
      },
      "source": [
        "## Functions and Classes\n",
        "*  Remove the punctuations\n",
        "*  Lower the cases\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fy2bDYorrKxW"
      },
      "outputs": [],
      "source": [
        "def get_corpus():\n",
        "  \"\"\" Reads and formats the corpus.\n",
        "\n",
        "  Returns:\n",
        "    corpus (list[str]):\n",
        "      A list of sentences in the corpus.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/yilihsu/NLP110/main/data_tiny.csv')\n",
        "  corpus = df.content.to_list()\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cqW8SUq_xzCm"
      },
      "outputs": [],
      "source": [
        "def preprocess(documents):\n",
        "  \"\"\" Preprocesses the corpus.\n",
        "  \n",
        "  Args:\n",
        "    documents (list[str]):\n",
        "      A list of sentences in the corpus.\n",
        "  Returns:\n",
        "    cleaned_documents (list[str]):\n",
        "      A list of cleaned sentences in the corpus.\n",
        "  \"\"\"\n",
        "  cleaned_documents = []\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~”'''\n",
        "  for doc in documents:\n",
        "    # Tokenizes the sentence\n",
        "    sents = sent_tokenize(doc)\n",
        "    \n",
        "    for sent in sents:\n",
        "      #pdb.set_trace() # delete this line for the final version\n",
        "      \n",
        "      # Removes the punctuations [TODO]\n",
        "      for cha in punc:\n",
        "        sent = sent.replace(cha,'')\n",
        "\n",
        "      # Lowers the case\n",
        "      sent = sent.lower() \n",
        "      \n",
        "      cleaned_documents.append(sent)\n",
        "\n",
        "  #print(cleaned_documents[:5])\n",
        "  return cleaned_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LLxv8uXpxvC2"
      },
      "outputs": [],
      "source": [
        "# Compute word frequency\n",
        "def get_vocab(documents):\n",
        "  \"\"\" Gets the vocabulary from the corpus.\n",
        "  \n",
        "  Args:\n",
        "    documents (list[str]):\n",
        "      A list of sentences in the corpus\n",
        "  Returns:\n",
        "    vocabulary (collections.Counter)\n",
        "  \"\"\"\n",
        "  vocabulary = Counter()\n",
        "\n",
        "  for doc in tqdm(documents):\n",
        "    tokens = word_tokenize(doc)\n",
        "    vocabulary.update(tokens)\n",
        "\n",
        "  return vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Au2w1vJjHqP"
      },
      "source": [
        "## Executions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkCSSWp3j65G"
      },
      "source": [
        "### 1. Show the top-10 common words and their counts before/after preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEDgtK3RxFXt",
        "outputId": "1cd4a35b-d35a-444e-9b85-fbdcbdb065f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20000/20000 [00:04<00:00, 4555.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Before preprocessing: [('.', 17263), ('the', 9880), (',', 7788), ('to', 7003), ('!', 6642), ('a', 5590), ('is', 5102), ('?', 4640), ('and', 4573), ('you', 4448)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35017/35017 [00:03<00:00, 9163.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " After preprocesing: [('the', 11175), ('to', 7117), ('a', 5847), ('you', 5324), ('is', 5245), ('and', 5087), ('of', 4492), ('i', 3231), ('in', 3203), ('it', 3187)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Read data\n",
        "raw_documents = get_corpus()\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = get_vocab(raw_documents).most_common(10)\n",
        "print('\\n Before preprocessing:', vocab)\n",
        "\n",
        "# Build vocabulary after preprocessing\n",
        "documents = preprocess(raw_documents)\n",
        "vocab = get_vocab(documents).most_common(10)\n",
        "print('\\n After preprocesing:', vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-mCeiF3inh-"
      },
      "source": [
        "# Part 2. N-Gram Model and POS Tagging\n",
        "1. Build 2-gram / 4-gram model by processed dataset\n",
        "2. Show the top-5 probable next words and their probability after initial token ‘\\<s\\>’ by 2-gram model\n",
        "3. Generate a sentence with 2-gram model and find the POS taggings\n",
        "4. Generate a sentence with 4-gram model and find the POS taggings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7MRk6EMjlKL"
      },
      "source": [
        "## Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nsStQG516jC6"
      },
      "outputs": [],
      "source": [
        "class Ngram_model(object):\n",
        "  \"\"\" Ngram model implementation.\n",
        "\n",
        "  Attributes:\n",
        "    n (int):\n",
        "      The number of grams to be considered.\n",
        "    model (dict):\n",
        "      The ngram model.\n",
        "  \"\"\"\n",
        "  def __init__(self, documents, N=2):\n",
        "    self.n = N\n",
        "    self.model = self.get_ngram_model(documents)\n",
        "\n",
        "  def get_ngram_model(self, documents):\n",
        "    N = self.n\n",
        "    ngram_model = dict()\n",
        "    full_grams = list()\n",
        "    grams = list()\n",
        "    Word = namedtuple('Word', ['word', 'prob'])\n",
        "\n",
        "    # for each sentence in documents\n",
        "    for document in documents:\n",
        "      \n",
        "      # Tokenizes to words\n",
        "      token = word_tokenize(document)\n",
        "      \n",
        "      # Append (N-1) start tokens '<s>' and an end token '<\\s>'\n",
        "      for i in range(N-1):\n",
        "        token = ['<s>'] + token + ['<\\\\s>']\n",
        "\n",
        "      # Calculates numerator (construct list with full grams, i.e., N-grams)\n",
        "      for i in range(len(token)-N+1):\n",
        "        full_grams.append(tuple(token[i:i+N]))\n",
        "\n",
        "      # Calculate denominator (construct list with grams, i.e., (N-1)-grams)\n",
        "      for i in range(len(token)-N+2):\n",
        "        grams.append(tuple(token[i:i+N-1]))\n",
        "      \n",
        "    # Count the occurence frequency of each gram\n",
        "    # Take 2-gram model as example:\n",
        "    #   full_grams -> list[('a', 'gram'),('other', 'gram'), ...]\n",
        "    #   grams -> list[('a'), ('other'), ('gram'), ...]\n",
        "    #   full_gram_counter -> dict{('a', 'gram'):frequency_1, ('other','gram'):frequency_2, ...}\n",
        "    #   gram_counter -> dict{('a'):frequency_1, ('gram'):frequency_2, ...}\n",
        "    full_gram_counter = Counter(full_grams)\n",
        "    gram_counter = Counter(grams)\n",
        "\n",
        "    # Build model\n",
        "    # Take 2-gram model as example:\n",
        "    #   { '<s>': [tuple(word='i', prob=0.6), tuple(word='the', prob=0.2), ...],\n",
        "    #   'i': [tuple(word='am', prob=0.7), tuple(word='want', prob=0.1), ...],\n",
        "    #    ... }\n",
        "    for key in full_gram_counter:\n",
        "      word = ''.join(key[:N-1])\n",
        "\n",
        "      if word not in ngram_model:\n",
        "        ngram_model.update({word: set()})\n",
        "\n",
        "      # next_word_prob -> float\n",
        "      next_word_prob = full_gram_counter[key] / gram_counter[key[:N-1]]\n",
        "      w = Word(key[-1], next_word_prob)\n",
        "      ngram_model[word].add(w)\n",
        "\n",
        "    # Sort the result by frequency\n",
        "    for word, ng in ngram_model.items():\n",
        "      ngram_model[word] = sorted(ng, key=lambda x: x.prob, reverse=True)\n",
        "\n",
        "    return ngram_model\n",
        "\n",
        "\n",
        "  def predict_sent(self, text=None, max_len=30):\n",
        "    \"\"\" Predicts a sentence with the ngram model.\n",
        "\n",
        "    Args:\n",
        "      text (string or list[string])\n",
        "    Returns:\n",
        "      A prediction string.\n",
        "    \"\"\"\n",
        "\n",
        "    N = self.n\n",
        "    backup_tokens = ['<s>']*(N-1)\n",
        "    if not text:\n",
        "      tokens = backup_tokens\n",
        "      output = []\n",
        "\n",
        "    elif type(text)==str:\n",
        "      tokens = backup_tokens + text.split(' ')\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return \n",
        "      output = tokens\n",
        "\n",
        "    elif type(text) == list:\n",
        "      tokens = backup_tokens + text\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return\n",
        "      output = tokens\n",
        "\n",
        "    else:\n",
        "      print('[Error] the input text must be string or list of string')\n",
        "      return\n",
        "\n",
        "    for i in range(max_len):\n",
        "      possible_words = list(self.model[''.join(tokens)])\n",
        "      probs = [word.prob for word in possible_words]\n",
        "      words = [word.word for word in possible_words]\n",
        "      next_word = np.random.choice(words, 1, p=probs)[0]\n",
        "      tokens = tokens[1:] + [next_word]\n",
        "\n",
        "      if next_word == '<\\\\s>':\n",
        "        break\n",
        "\n",
        "      output.append(next_word)\n",
        "    return ' '.join(output)\n",
        "\n",
        "  def predict_next(self, text=None, top=5):\n",
        "    \"\"\" Predicts next word with the ngram model.\n",
        "\n",
        "    Args:\n",
        "      text (string or list[string])\n",
        "\n",
        "    Returns:\n",
        "      possible_next_words (list[namedtuple]):\n",
        "        A list of top few possible next words.\n",
        "    \"\"\"\n",
        "\n",
        "    N = self.n\n",
        "    backup_tokens = ['<s>']*(N-1)\n",
        "    if not text:\n",
        "      tokens = backup_tokens\n",
        "\n",
        "    elif type(text)==str:\n",
        "      tokens = backup_tokens + text.split(' ')\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return \n",
        "\n",
        "    elif type(text) == list:\n",
        "      tokens = backup_tokens + text\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return\n",
        "    else:\n",
        "      print('[Error] the input text must be string or list of string')\n",
        "\n",
        "    possible_next_words = self.model[''.join(tokens)][:top]\n",
        "    possible_next_words = [(word.word, word.prob) for word in possible_next_words]\n",
        "\n",
        "    return possible_next_words\n",
        "\n",
        "  def check_existence(self, tokens):\n",
        "    if not ''.join(tokens) in self.model.keys():\n",
        "      print('[Error] the input text {} not in the vocabulary'.format(tokens))\n",
        "      return False\n",
        "    else:\n",
        "      return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_CgC8RKjhd0"
      },
      "source": [
        "## Executions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcJhv21YCE8I"
      },
      "source": [
        "### 1. Build 2-gram/4-gram model by processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "h17uKROx-FZG"
      },
      "outputs": [],
      "source": [
        "twogram = Ngram_model(documents, N=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOV203H4CNg7"
      },
      "source": [
        "### 2. Show the top-5 probable next words and their probability after initial token \\'\\<s\\>\\'  by 2-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CoD7wcAJ-PYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbfc970-796f-4cc0-f479-27ca2b49734a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word predictions of two gram model: [('i', 0.059228374789388014), ('you', 0.034383299540223324), ('the', 0.030984950167061712), ('<\\\\s>', 0.030013993203301254), ('they', 0.021532398549276067)]\n"
          ]
        }
      ],
      "source": [
        "output = twogram.predict_next(text='<s>', top=5)\n",
        "print('Next word predictions of two gram model:', output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C51gJXJO7JBi"
      },
      "source": [
        "### 3. Generate a sentence with 2-gram model and find the POS taggings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CBvwvE6q7Vpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2100b5-f6b5-4794-9700-52cabbe3c64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation results of two gram model: i think of the first respondersimagine the side of healthy then give me\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'JJ'),\n",
              " ('think', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('first', 'JJ'),\n",
              " ('respondersimagine', 'NN'),\n",
              " ('the', 'DT'),\n",
              " ('side', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('healthy', 'JJ'),\n",
              " ('then', 'RB'),\n",
              " ('give', 'VB'),\n",
              " ('me', 'PRP')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "output = twogram.predict_sent(max_len=30)\n",
        "print('Generation results of two gram model:', output)\n",
        "nltk.pos_tag(word_tokenize(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "W-n6loDYC8qn"
      },
      "outputs": [],
      "source": [
        "def evaluation(generated_sentence, reference_sentence):\n",
        "\n",
        "  if len(generated_sentence) <= 1: \n",
        "    raise RuntimeError('Not enough length to evaluate, please try again with another generation.')\n",
        "\n",
        "  rouge = Rouge()\n",
        "  smoothie = SmoothingFunction().method4\n",
        "  \n",
        "  # Hint: please refer to the import function in the beginning of this notebook\n",
        "  # Given the smoothing_function=smoothie, weights=(1,0,0,0), please calculate BLEU-1 score with function call\n",
        "  bleu_score = sentence_bleu(reference_sentence,generated_sentence,weights=(1,0,0,0),smoothing_function=smoothie)\n",
        "  # Calculates ROUGE-1 f score with function call\n",
        "  rouge_score = rouge.get_scores(\" \".join(generated_sentence),\" \".join(reference_sentence))[0]['rouge-1']['f']\n",
        "  \n",
        "  return bleu_score, rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iYA9Yh8qEhEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf60900-dfab-4b05-df83-1d05f47b4f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated results given the text:  ['is', 'the', 'constitutional', 'changes', 'those', 'goons', 'but', 'i', 'agree', 'with', 'no', 'shame', 'on', 'the', 'charity']\n",
            "Reference sentence:  ['is', 'a', 'party', 'day']\n",
            "bleu score:  0.06666666666666667\n",
            "rouge score:  0.1111111076543211\n"
          ]
        }
      ],
      "source": [
        "given_text = \"today is\"\n",
        "references = ['is', 'a', 'party', 'day']\n",
        "output = twogram.predict_sent(text=given_text, max_len=30)\n",
        "output = word_tokenize(output)\n",
        "print(\"Generated results given the text: \", output)\n",
        "print(\"Reference sentence: \", references)\n",
        "\n",
        "bleu_score, rouge_score = evaluation(output, references)\n",
        "print(\"bleu score: \", bleu_score)\n",
        "print(\"rouge score: \", rouge_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pWCalWsiHWBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab_1_309833027.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}